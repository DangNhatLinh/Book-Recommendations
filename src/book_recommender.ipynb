{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "93c552aa-0827-450f-b80f-bb58ddccf126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import re, os, glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d884d8c5-ba49-43f7-9f7c-1d1835850cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_ONLY = re.compile(r\"[a-z]+\") # storing only words cuz it's more convenient to analyze with words only\n",
    "\n",
    "def load_text(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read().replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\") # replace \\r\\n with \\n and then replace \\r with \\n\n",
    "\n",
    "def load_docs(pattern=\"../data/*.txt\"): # all of my files begin end w .txt in data\n",
    "    return {Path(p).stem: load_text(p) for p in glob.glob(pattern)}\n",
    "\n",
    "docs = {Path(p).stem: load_text(p) for p in glob.glob(\"../data/*.txt\")}\n",
    "tokens = {name: words_only(txt) for name, txt in docs.items()}\n",
    "\n",
    "def words_only(text: str):\n",
    "    text = text.lower()\n",
    "    return WORDS_ONLY.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "cd1a488e-cf84-440f-a9e5-91b2f1a6726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = re.compile(r\"\\*\\*\\*\\s*START OF (?:THE|THIS) PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\", re.I|re.S) # strip header\n",
    "END   = re.compile(r\"\\*\\*\\*\\s*END OF (?:THE|THIS) PROJECT GUTENBERG EBOOK.*\", re.I|re.S) # strip footer\n",
    "TOKEN  = re.compile(r\"[a-z']+\") # tokenization 1st step\n",
    "RAW_CONTENT = re.compile(\n",
    "    r\"^(?:preface|chapter\\s+(?:\\d+|[ivxlcdm]+))\\b\", # ivxlcdm is normal AND roman numbers\n",
    "    flags=re.I | re.M\n",
    ") # most books start with preface or chapter x\n",
    "HEAD_START = re.compile(\n",
    "    r\"^\\s*(chapter\\b|contents\\b|epilogue\\b|preface\\b|prologue\\b|etymology\\b)\",\n",
    "    re.I\n",
    ") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4ea43cb2-41b0-4f39-b267-f08c99dd7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_content(t: str) -> str: # strip the gutenberg start and end points\n",
    "    t = t.replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")\n",
    "    m = START.search(t)\n",
    "    if m: \n",
    "        t = t[m.end():]\n",
    "    m = END.search(t)\n",
    "    if m: \n",
    "        t = t[:m.start()]\n",
    "    return t.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "e5d22a8b-c7ff-4110-8143-b4f630086a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def starting_point(t: str) -> str: # start at preface or chapter sth sth line\n",
    "    m = RAW_CONTENT.search(t)\n",
    "    if m:\n",
    "        line_start = t.rfind(\"\\n\", 0, m.start()) + 1 # teleport to the start of the line\n",
    "        return t[line_start:].lstrip() # if cant find the keyword preface or chapter, return to the original\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f5fc2670-6b19-48ea-8cd4-73f0e2280ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_removals(text: str) -> str:\n",
    "    kept = []\n",
    "    for ln in text.splitlines():\n",
    "        if HEAD_START.match(ln.strip()):\n",
    "            continue\n",
    "        kept.append(ln)\n",
    "    return re.sub(r\"\\n{3,}\", \"\\n\\n\", \"\\n\".join(kept)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c7baf9bb-fb39-4331-aa26-b866801d1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACTS = {\"chapter\",\"chap\",\"book\",\"preface\",\"contents\",\"page\",\n",
    "             \"project\",\"gutenberg\",\"ebook\",\"transcriber\",\"pgdp\", \"illustration\", \"copyright\", \"'\"}\n",
    "tokens = [t for t in tokens if t not in ARTIFACTS]\n",
    "def remove_transcriber(text: str) -> str:\n",
    "    bigNO = (\"transcriber\", \"transcriber's note\", \"proofreading\", \"pgdp\", \"proofreaders\", \"illustration\", \"copyright\", \"'\")\n",
    "    return \"\\n\".join(ln for ln in text.splitlines()\n",
    "                     if not any(b in ln.lower() for b in bigNO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "29f42dfc-18e0-4e75-a780-c7ad6bf90cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(body: str): # basic tokenization of the body explained in the handout\n",
    "    body = body.lower()\n",
    "    body = re.sub(r\"[^a-z\\s']\", \" \", body)\n",
    "    return TOKEN.findall(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "fef405b5-b04f-4727-8c3b-f553dc8a4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data\n",
    "def clean(raw: str):\n",
    "    body = strip_content(raw)\n",
    "    body = starting_point(body)  # drop everything before preface OR chapter cuz we dont want them\n",
    "    body = additional_removals(body)\n",
    "    body = remove_transcriber(body)\n",
    "    toks = tokenize(body)\n",
    "    return body, toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "daf29876-9f7f-4148-932f-15c4780865b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting words for each book by dictionary\n",
    "pattern = \"../data/*.txt\"\n",
    "tokens_by_book = {}\n",
    "bodies_by_book = {}\n",
    "\n",
    "for p in glob.glob(pattern):\n",
    "    name = Path(p).stem\n",
    "    raw = load_text(p)\n",
    "    body, toks = clean(raw)\n",
    "    bodies_by_book[name] = body\n",
    "    tokens_by_book[name] = toks\n",
    "\n",
    "book_names  = sorted(tokens_by_book.keys())\n",
    "token_lists = [tokens_by_book[n] for n in book_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "450a6a5b-472c-47ae-9bd5-623fc7d74d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# tokens_by_book: {doc: [w1, w2, ...]}\n",
    "# build TF\n",
    "rows = []\n",
    "for doc, toks in tokens_by_book.items():\n",
    "    for term, cnt in Counter(toks).items():\n",
    "        rows.append((doc, term, cnt))\n",
    "counts = pd.DataFrame(rows, columns=[\"doc\",\"term\",\"count\"])\n",
    "\n",
    "tf = counts.pivot_table(index=\"doc\", columns=\"term\", values=\"count\", fill_value=0).astype(float)\n",
    "tf = tf.div(tf.sum(axis=1).replace(0,1), axis=0)\n",
    "\n",
    "# IDF (smoothed)\n",
    "N  = tf.shape[0]\n",
    "df = (tf > 0).sum(axis=0).astype(float)\n",
    "idf = np.log(N / (1.0 + df))\n",
    "\n",
    "tfidf = tf.mul(idf, axis=1)  # <-- your model uses ALL terms (names included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "048b2c97-ccfe-45cb-8b78-d6575e58794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_terms(tfidf_df, df_series, topk=10, min_df=1, max_df=None, must_include=None, must_exclude=None):\n",
    "    \"\"\"\n",
    "    Only for display: choose terms by doc-frequency band, optionally exclude some tokens.\n",
    "    Data in tfidf_df remains unchanged.\n",
    "    \"\"\"\n",
    "    mask = (df_series >= min_df)\n",
    "    if max_df is not None:\n",
    "        mask &= (df_series <= max_df)\n",
    "    terms = df_series.index[mask]\n",
    "\n",
    "    if must_exclude:\n",
    "        terms = [t for t in terms if t not in must_exclude]\n",
    "    if must_include:  # ensure certain topic terms always allowed\n",
    "        terms = sorted(set(terms) | set(must_include))\n",
    "\n",
    "    for doc in tfidf_df.index:\n",
    "        s = tfidf_df.loc[doc, terms].nlargest(topk)\n",
    "        print(f\"\\n=== {doc} ===\")\n",
    "        for term, score in s.items():\n",
    "            print(f\"{term:20s} {score:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "842fdebf-4d9e-4959-8378-ab850d9d85ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Alice's Adventures in Wonderland ===\n",
      "alice                0.01889\n",
      "duchess              0.00200\n",
      "turtle               0.00170\n",
      "rabbit               0.00147\n",
      "caterpillar          0.00133\n",
      "mock                 0.00093\n",
      "hare                 0.00089\n",
      "jury                 0.00081\n",
      "mouse                0.00073\n",
      "dodo                 0.00062\n",
      "\n",
      "=== Beowulf- An Anglo-Saxon Epic Poem ===\n",
      "folk                 0.00192\n",
      "armor                0.00123\n",
      "battle               0.00121\n",
      "danes                0.00120\n",
      "heroes               0.00118\n",
      "warriors             0.00110\n",
      "sword                0.00110\n",
      "atheling             0.00104\n",
      "dragon               0.00103\n",
      "thou                 0.00095\n",
      "\n",
      "=== Dracula ===\n",
      "van                  0.00256\n",
      "lucy                 0.00238\n",
      "jonathan             0.00160\n",
      "arthur               0.00117\n",
      "diary                0.00079\n",
      "professor            0.00065\n",
      "morris               0.00062\n",
      "dr                   0.00049\n",
      "whilst               0.00038\n",
      "wolves               0.00037\n",
      "\n",
      "=== Frankenstein; Or, The Modern Prometheus ===\n",
      "elizabeth            0.00104\n",
      "geneva               0.00067\n",
      "cottage              0.00044\n",
      "cottagers            0.00043\n",
      "agatha               0.00041\n",
      "creator              0.00037\n",
      "endeavoured          0.00031\n",
      "murderer             0.00031\n",
      "wretch               0.00028\n",
      "hovel                0.00026\n",
      "\n",
      "=== Grimms' Fairy Tales ===\n",
      "hans                 0.00159\n",
      "dwarf                0.00079\n",
      "forest               0.00066\n",
      "princess             0.00061\n",
      "fox                  0.00053\n",
      "prince               0.00040\n",
      "peasant              0.00040\n",
      "cat                  0.00039\n",
      "frederick            0.00033\n",
      "wolf                 0.00032\n",
      "\n",
      "=== Little Women; Or, Meg, Jo, Beth, and Amy ===\n",
      "novel                0.00041\n",
      "illustrated          0.00036\n",
      "demi                 0.00035\n",
      "mrs                  0.00026\n",
      "parlor               0.00026\n",
      "daisy                0.00023\n",
      "aunt                 0.00022\n",
      "professor            0.00021\n",
      "kate                 0.00019\n",
      "babies               0.00017\n",
      "\n",
      "=== Moby Dick; Or, The Whale ===\n",
      "whale                0.00725\n",
      "sperm                0.00144\n",
      "mast                 0.00076\n",
      "aye                  0.00072\n",
      "thou                 0.00056\n",
      "jonah                0.00050\n",
      "boat                 0.00049\n",
      "ship                 0.00048\n",
      "pip                  0.00044\n",
      "ye                   0.00044\n",
      "\n",
      "=== Nora's twin sister ===\n",
      "sarah                0.00510\n",
      "crawford             0.00257\n",
      "marjorie             0.00201\n",
      "daddy                0.00188\n",
      "mummy                0.00164\n",
      "didn                 0.00095\n",
      "beck                 0.00090\n",
      "jimmy                0.00080\n",
      "isn                  0.00076\n",
      "wouldn               0.00075\n",
      "\n",
      "=== Pride and Prejudice ===\n",
      "elizabeth            0.00397\n",
      "charlotte            0.00088\n",
      "gardiner             0.00077\n",
      "catherine            0.00063\n",
      "mr                   0.00060\n",
      "kitty                0.00057\n",
      "mrs                  0.00055\n",
      "colonel              0.00054\n",
      "ladyship             0.00033\n",
      "behaviour            0.00026\n",
      "\n",
      "=== Sense and Sensibility ===\n",
      "willoughby           0.00233\n",
      "dashwood             0.00211\n",
      "lucy                 0.00200\n",
      "edward               0.00172\n",
      "colonel              0.00148\n",
      "mrs                  0.00088\n",
      "fanny                0.00040\n",
      "behaviour            0.00032\n",
      "robert               0.00031\n",
      "cottage              0.00031\n",
      "\n",
      "=== Wuthering Heights ===\n",
      "catherine            0.00194\n",
      "joseph               0.00152\n",
      "edgar                0.00098\n",
      "isabella             0.00069\n",
      "ellen                0.00065\n",
      "ll                   0.00062\n",
      "heights              0.00050\n",
      "papa                 0.00038\n",
      "didn                 0.00026\n",
      "mr                   0.00025\n"
     ]
    }
   ],
   "source": [
    "print_top_terms(tfidf, df, topk=10, min_df=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b83aa-544a-46d3-8456-b2c59b2c6244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
