{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "93c552aa-0827-450f-b80f-bb58ddccf126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import re, os, glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d884d8c5-ba49-43f7-9f7c-1d1835850cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_ONLY = re.compile(r\"[a-z]+\") # storing only words cuz it's more convenient to analyze with words only\n",
    "\n",
    "def load_text(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read().replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\") # replace \\r\\n with \\n and then replace \\r with \\n\n",
    "\n",
    "def load_docs(pattern=\"../data/*.txt\"): # all of my files begin end w .txt in data\n",
    "    return {Path(p).stem: load_text(p) for p in glob.glob(pattern)}\n",
    "\n",
    "docs = {Path(p).stem: load_text(p) for p in glob.glob(\"../data/*.txt\")}\n",
    "tokens = {name: words_only(txt) for name, txt in docs.items()}\n",
    "\n",
    "def words_only(text: str):\n",
    "    text = text.lower()\n",
    "    return WORDS_ONLY.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cd1a488e-cf84-440f-a9e5-91b2f1a6726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = re.compile(r\"\\*\\*\\*\\s*START OF (?:THE|THIS) PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\", re.I|re.S) # strip header\n",
    "END   = re.compile(r\"\\*\\*\\*\\s*END OF (?:THE|THIS) PROJECT GUTENBERG EBOOK.*\", re.I|re.S) # strip footer\n",
    "TOKEN  = re.compile(r\"[a-z0-9']+\") # tokenization 1st step\n",
    "RAW_CONTENT = re.compile(\n",
    "    r\"^(?:preface|chapter\\s+(?:\\d+|[ivxlcdm]+))\\b\", # ivxlcdm is normal AND roman numbers\n",
    "    flags=re.I | re.M\n",
    ") # most books start with preface or chapter x\n",
    "HEAD_START = re.compile(\n",
    "    r\"^\\s*(chapter\\b|contents\\b|epilogue\\b|preface\\b|prologue\\b|etymology\\b)\",\n",
    "    re.I\n",
    ") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4ea43cb2-41b0-4f39-b267-f08c99dd7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_content(t: str) -> str: # strip the gutenberg start and end points\n",
    "    t = t.replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")\n",
    "    m = START.search(t)\n",
    "    if m: \n",
    "        t = t[m.end():]\n",
    "    m = END.search(t)\n",
    "    if m: \n",
    "        t = t[:m.start()]\n",
    "    return t.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e5d22a8b-c7ff-4110-8143-b4f630086a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def starting_point(t: str) -> str: # start at preface or chapter sth sth line\n",
    "    m = RAW_CONTENT.search(t)\n",
    "    if m:\n",
    "        line_start = t.rfind(\"\\n\", 0, m.start()) + 1 # teleport to the start of the line\n",
    "        return t[line_start:].lstrip() # if cant find the keyword preface or chapter, return to the original\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f5fc2670-6b19-48ea-8cd4-73f0e2280ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_removals(text: str) -> str:\n",
    "    kept = []\n",
    "    for ln in text.splitlines():\n",
    "        if HEAD_START.match(ln.strip()):\n",
    "            continue\n",
    "        kept.append(ln)\n",
    "    return re.sub(r\"\\n{3,}\", \"\\n\\n\", \"\\n\".join(kept)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c7baf9bb-fb39-4331-aa26-b866801d1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACTS = {\"chapter\",\"chap\",\"book\",\"preface\",\"contents\",\"page\",\n",
    "             \"project\",\"gutenberg\",\"ebook\",\"transcriber\",\"pgdp\"}\n",
    "tokens = [t for t in tokens if t not in ARTIFACTS]\n",
    "def remove_transcriber(text: str) -> str:\n",
    "    bigNO = (\"transcriber\", \"transcriber's note\", \"proofreading\", \"pgdp\", \"proofreaders\")\n",
    "    return \"\\n\".join(ln for ln in text.splitlines()\n",
    "                     if not any(b in ln.lower() for b in bigNO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "29f42dfc-18e0-4e75-a780-c7ad6bf90cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(body: str): # basic tokenization of the body explained in the handout\n",
    "    body = body.lower()\n",
    "    body = re.sub(r\"[^a-z0-9\\s']\", \" \", body)\n",
    "    return TOKEN.findall(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fef405b5-b04f-4727-8c3b-f553dc8a4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data\n",
    "def clean(raw: str):\n",
    "    body = strip_content(raw)\n",
    "    body = starting_point(body)  # drop everything before preface OR chapter cuz we dont want them\n",
    "    body = additional_removals(body)\n",
    "    body = remove_transcriber(body)\n",
    "    toks = tokenize(body)\n",
    "    return body, toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "daf29876-9f7f-4148-932f-15c4780865b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting words for each book by dictionary\n",
    "pattern = \"../data/*.txt\"\n",
    "tokens_by_book = {}\n",
    "bodies_by_book = {}\n",
    "\n",
    "for p in glob.glob(pattern):\n",
    "    name = Path(p).stem\n",
    "    raw = load_text(p)\n",
    "    body, toks = clean(raw)\n",
    "    bodies_by_book[name] = body\n",
    "    tokens_by_book[name] = toks\n",
    "\n",
    "book_names  = sorted(tokens_by_book.keys())\n",
    "token_lists = [tokens_by_book[n] for n in book_names]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
