{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93c552aa-0827-450f-b80f-bb58ddccf126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import re, os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d884d8c5-ba49-43f7-9f7c-1d1835850cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 files: []\n"
     ]
    }
   ],
   "source": [
    "def load_text(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read().replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")\n",
    "\n",
    "def load_docs(pattern=\"data/*.txt\"):\n",
    "    return {Path(p).stem: load_text(p) for p in glob.glob(pattern)}\n",
    "\n",
    "docs = load_docs(\"data/*.txt\")\n",
    "print(f\"Loaded {len(docs)} files:\", list(docs)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd1a488e-cf84-440f-a9e5-91b2f1a6726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "START = re.compile(r\"\\*\\*\\*\\s*START OF (?:THE|THIS) PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\", re.I|re.S) # strip header\n",
    "END   = re.compile(r\"\\*\\*\\*\\s*END OF (?:THE|THIS) PROJECT GUTENBERG EBOOK.*\", re.I|re.S) # strip footer\n",
    "TOKEN  = re.compile(r\"[a-z0-9']+\") # tokenization 1st step\n",
    "RAW_CONTENT = re.compile(\n",
    "    r\"^(?:preface|chapter\\s+(?:\\d+|[ivxlcdm]+))\\b\", # ivxlcdm is normal AND roman numbers\n",
    "    flags=re.I | re.M\n",
    ") # most books start with preface or chapter x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ea43cb2-41b0-4f39-b267-f08c99dd7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_content(t: str) -> str: # strip the gutenberg start and end points\n",
    "    t = t.replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")\n",
    "    m = START.search(t)\n",
    "    if m: \n",
    "        t = t[m.end():]\n",
    "    m = END.search(t)\n",
    "    if m: \n",
    "        t = t[:m.start()]\n",
    "    return t.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5d22a8b-c7ff-4110-8143-b4f630086a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def starting_point(t: str) -> str: # start at preface or chapter sth sth line\n",
    "    m = RAW_CONTENT.search(t)\n",
    "    if m:\n",
    "        line_start = t.rfind(\"\\n\", 0, m.start()) + 1 # teleport to the start of the line\n",
    "        return t[line_start:].lstrip() # if cant find the keyword preface or chapter, return to the original\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29f42dfc-18e0-4e75-a780-c7ad6bf90cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(body: str): # basic tokenization of the body explained in the handout\n",
    "    body = body.lower()\n",
    "    body = re.sub(r\"[^a-z0-9\\s']\", \" \", body)\n",
    "    return TOKEN.findall(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fef405b5-b04f-4727-8c3b-f553dc8a4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data\n",
    "def clean(raw: str):\n",
    "    body = strip_content(raw)\n",
    "    body = starting_point(body)  # drop everything before preface OR chapter cuz we dont want them\n",
    "    toks = tokenize(body)\n",
    "    return body, toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "daf29876-9f7f-4148-932f-15c4780865b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bodies, tokens = {}, {}\n",
    "for name, raw in docs.items():\n",
    "    body, toks = clean(raw)\n",
    "    bodies[name] = body\n",
    "    tokens[name] = toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03f31697-8a19-4c12-8f6a-b872970c7d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in list(bodies)[:2]:\n",
    "    print(\"â€”\", name, \"| body chars:\", len(bodies[name]), \"| tokens:\", len(tokens[name]))\n",
    "    print(bodies[name][:200], \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a744e2-7af7-4eb9-9ebc-d709553cb1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
